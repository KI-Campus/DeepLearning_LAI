{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEgUdrADq8cF"
   },
   "source": [
    "# Deep Learning\n",
    "## Exercise 3 - Neural Networks and Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hp8yuc6mLHKU"
   },
   "source": [
    "### 0. Neural Networks from Scratch\n",
    "Before working on the code assignments in this notebook, we recommend reading through the **What is torch.nn really?** [tutorial notebook](https://pytorch.org/tutorials/beginner/nn_tutorial.html). \n",
    "\n",
    "For this exercise, it is enough to work through the *Neural net from scratch (no torch.nn)* section. The *Refactor using ...* sections will be important for the next exercise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vCVq9AEfq8cJ"
   },
   "source": [
    "### 1. Learning XOR\n",
    "The XOR function is defined as\n",
    "\\begin{equation}\n",
    "    x_1 \\oplus x_2 =\n",
    "    \\begin{cases}\n",
    "        0 \\quad x_1 = x_2\\\\\n",
    "        1 \\quad x_1 \\neq x_2\\\\\n",
    "    \\end{cases}.\n",
    "\\end{equation}\n",
    "\n",
    "In this task we want to use [`scikit-learn`](https://scikit-learn.org/stable/index.html) to train a linear model on the XOR function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. What are the possible input-output-pairs $(x, y)$?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# ToDo: define input-output pairs\n",
    "X = torch.tensor([])\n",
    "y = torch.tensor([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lFIxvjRmLHKV",
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "X = torch.tensor([[0,0], [0,1], [1,0], [1,1]],dtype=float)\n",
    "y = torch.tensor([0, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Use [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) to train a linear classifier on all pairs from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# ToDo: Fit a linear regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "lrm = LinearRegression()\n",
    "lrm = lrm.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. Test your model. Is it working well? Why (not)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ToDo: Make predicitons and test/visualize the linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "print(lrm.score(X,y))\n",
    "\n",
    "y_hat = lrm.predict(X)\n",
    "\n",
    "print(\"X \\ty \\ty_hat\")\n",
    "for x_i, y_i, y_hat_i in zip(X, y, y_hat):\n",
    "    print(f\"{np.array(x_i)}\\t{y_i}\\t{y_hat_i:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVuVHXb6LHKW"
   },
   "source": [
    "### 2. Learning XOR with MLP\n",
    "We want to implement a simple feed-forward net that can learn the XOR function. Our network should have a single $5$-dimensional hidden layer $h$ and use sigmoid activation in all layers. Set the learning rate to $\\eta = 1$.\n",
    "\n",
    "\n",
    "For this task, use only basic tensor features of **PyTorch**. Specifically, do **not** use \n",
    "* autograd , e.g. `.backward()`\n",
    "* `torch.nn`\n",
    "* `torch.optim`\n",
    "* the built-in sigmoid function (implement it yourself)\n",
    "* or similar off-the-shelf building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Initialize the weight matrices.\n",
    "\n",
    "What are the dimensions of the two weight matrices? Initialize the weight matrices randomly in $[-1, 1]$. You can omit the bias terms in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "g = torch.Generator()\n",
    "g.manual_seed(1234567)\n",
    "\n",
    "def initialize_weights(g):\n",
    "    \"\"\"\n",
    "    Initialize the weight matrices randomly in [-1, 1].\n",
    "    \n",
    "    Input values:\n",
    "        g : random number generator (used for reproducability)\n",
    "        \n",
    "    Output values:\n",
    "        w_1 : weight matrix of shape (a, b)\n",
    "        w_2 : weight matrix of shape (c, d)\n",
    "    \"\"\"\n",
    "    # ToDo: create and initialize the weight matrices.\n",
    "\n",
    "    return w_1, w_2\n",
    "\n",
    "w_1, w_2 = initialize_weights(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(g):\n",
    "    \"\"\"\n",
    "    Initialize the weight matrices randomly in [-1, 1].\n",
    "    \n",
    "    Input values:\n",
    "        g : random number generator (used for reproducability)\n",
    "        \n",
    "    Output values:\n",
    "        w_1 : weight matrix of shape (a, b)\n",
    "        w_2 : weight matrix of shape (c, d)\n",
    "    \"\"\"\n",
    "    w_1 = torch.rand((2,5),generator=g, dtype=float)*2-1\n",
    "    w_2 = torch.rand((5,1),generator=g, dtype=float)*2-1\n",
    "    \n",
    "    return w_1, w_2\n",
    "\n",
    "w_1, w_2 = initialize_weights(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Implement the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    \"\"\"\n",
    "    Calculate the sigmod activation for z.\n",
    "    \n",
    "    Input value:\n",
    "        z : torch.tensor of shape (batch_size, n)\n",
    "        \n",
    "    Output value:\n",
    "        sigmoid : the sigmoid activation of z\n",
    "    \"\"\"\n",
    "    #ToDo: Implement sigmoid activation\n",
    "    return sigmoid\n",
    "\n",
    "def forward(x, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Calculate the forward pass through the model.\n",
    "    \n",
    "    Input value:\n",
    "        x : the input data of shape (batch_size, x_1, x_2)\n",
    "        w_1 : the first weight matrix initialized\n",
    "        w_2 : the second weight matrix initialized\n",
    "        \n",
    "    Output value: \n",
    "        y_hat : the predicted value\n",
    "        z_1, h, z_2 :  the intermediate values\n",
    "    \"\"\"\n",
    "    #ToDo: Implement the forward pass\n",
    "    \n",
    "    return y_hat, z_1, h, z_2\n",
    "\n",
    "y_hat, z_1, h, z_2 = forward(torch.tensor([0.,0.],dtype=float), w_1, w_2)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sigmoid_activation(z):\n",
    "    \"\"\"\n",
    "    Calculate the sigmod activation for z.\n",
    "    \n",
    "    Input value:\n",
    "        z : torch.tensor of shape (batch_size, n)\n",
    "        \n",
    "    Output value:\n",
    "        sigmoid : the sigmoid activation of z\n",
    "    \"\"\"\n",
    "    sigmoid = 1 / (1 + torch.exp(-z))\n",
    "    \n",
    "    return sigmoid\n",
    "\n",
    "def forward(x, w_1, w_2):\n",
    "    \"\"\"\n",
    "    Calculate the forward pass through the model.\n",
    "    \n",
    "    Input value:\n",
    "        x : the input data of shape (batch_size, x_1, x_2)\n",
    "        w_1 : the first weight matrix initialized\n",
    "        w_2 : the second weight matrix initialized\n",
    "        \n",
    "    Output value: \n",
    "        y_hat : the predicted value\n",
    "        z_1, h, z_2 :  the intermediate values\n",
    "    \"\"\"\n",
    "    z_1 = w_1.T @ x\n",
    "    h = sigmoid_activation(z_1)\n",
    "    z_2 = w_2.T @ h\n",
    "    y_hat = sigmoid_activation(z_2)\n",
    "    \n",
    "    return y_hat, z_1, h, z_2\n",
    "\n",
    "y_hat, z_1, h, z_2 = forward(torch.tensor([0.,0.],dtype=float), w_1, w_2)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Implement the Loss function $SE(y, \\hat{y}) = \\frac{1}{2} (y - \\hat{y})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def sq_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Calculate the square loss.\n",
    "    \n",
    "    Input values:\n",
    "        y_hat : the predicted values of your model\n",
    "        y : the true labels\n",
    "    \"\"\"\n",
    "    #ToDo: Implement the loss function\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(sq_loss(y_hat, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def sq_loss(y_hat, y):\n",
    "    \"\"\"\n",
    "    Calculate the square loss.\n",
    "    \n",
    "    Input values:\n",
    "        y_hat : the predicted values of your model\n",
    "        y : the true labels\n",
    "    \"\"\"\n",
    "    loss = 1./2. * (y-y_hat).pow(2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(sq_loss(y_hat, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Implement a backward pass\n",
    "\n",
    "Compute the gradients of the weights w.r.t. the loss and update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def backward(x, y, y_hat, w_1, w_2, z_1, h, z_2, lr):\n",
    "    \"\"\"\n",
    "    Calculate the backward pass through the model.\n",
    "    \n",
    "    Input values:\n",
    "        x : the input to the model\n",
    "        y : the true labels\n",
    "        y_hat : the predicted values of the model\n",
    "        w_1, w_2 : the weight matrices\n",
    "        z_1, h, z_2 : the intermediate values of the model\n",
    "        lr : the learning rate\n",
    "        \n",
    "    Output values:\n",
    "        w_1, w_2 : the updated weight matrices\n",
    "    \"\"\"\n",
    "    \n",
    "    #ToDo: Implement the gradient calculations with respect to the weights and update the weights.\n",
    "    return w_1, w_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def backward(x, y, y_hat, w_1, w_2, z_1, h, z_2, lr):\n",
    "    \"\"\"\n",
    "    Calculate the backward pass through the model.\n",
    "    \n",
    "    Input values:\n",
    "        x : the input to the model\n",
    "        y : the true labels\n",
    "        y_hat : the predicted values of the model\n",
    "        w_1, w_2 : the weight matrices\n",
    "        z_1, h, z_2 : the intermediate values of the model\n",
    "        lr : the learning rate\n",
    "        \n",
    "    Output values:\n",
    "        w_1, w_2 : the updated weight matrices\n",
    "    \"\"\"\n",
    "\n",
    "    dLdy = (y_hat-y)\n",
    "    dydz2 = sigmoid_activation(z_2)*(1-sigmoid_activation(z_2))\n",
    "    dz2dw2 = h.T\n",
    "\n",
    "    \n",
    "    dLdw2 =  (dLdy * dydz2) @ dz2dw2\n",
    "\n",
    "    \n",
    "    dz2dh = w_2.T\n",
    "    dhdz1 = sigmoid_activation(z_1)*(1-sigmoid_activation(z_1))\n",
    "    dz1dw1 = torch.tensor([[[x[j].item() if i == k else 0 for k in range(5)] for j in range(2)] for i in range(5)])\n",
    "\n",
    "    dLdw1_1 =  ((dLdy * dydz2) @ dz2dh  * dhdz1.T) \n",
    "    print(dLdw1_1.shape, dz1dw1.shape)\n",
    "    dLdw1 = np.tensordot(dLdw1_1, dz1dw1, axes=(1,0))\n",
    "    print(dLdw1.shape)\n",
    "    print(w_2.shape, dLdw2.shape)\n",
    "    \n",
    "    w_1 -= dLdw1.squeeze() * lr\n",
    "    w_2 -= dLdw2.squeeze().unsqueeze(-1) * lr\n",
    "    print(w_1.shape, w_2.shape)\n",
    "\n",
    "    return w_1, w_2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "To implement the backward pass, we utilize the back-propagation algorithm. The idea is to find the derivative of the loss function w.r.t. each parameter of the model. We use the chain rule to accomplish this.\n",
    "\n",
    "\\begin{align}\n",
    " L &= \\frac{1}{2} (y - \\hat{y})^2  &\\in \\mathbb{R}\\\\\n",
    " \\hat{y} &= \\sigma(z_2) &\\in \\mathbb{R}^{1 \\times 1}\\\\\n",
    " z_2 &= W_2^T h  &\\in \\mathbb{R}^{1 \\times 1}\\\\\n",
    " h &= \\sigma(z_1) &\\in \\mathbb{R}^{5 \\times 1}\\\\\n",
    " z_1 &= W_1^T x &\\in \\mathbb{R}^{5 \\times 1}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "We can calculate the derivative of the loss w.r.t. the weight matrix $W_2$ as\n",
    "\\begin{align}\n",
    "    \\frac{\\partial L}{\\partial W_2} &= \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial W_2}\n",
    "\\end{align}\n",
    "\n",
    "The derivative $\\frac{\\partial L}{\\partial W_1}$ can be calculated similarly:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial L}{\\partial W_1} &= \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial z_2} \\frac{\\partial z_2}{\\partial h} \\frac{\\partial h}{\\partial z_1} \\frac{\\partial z_1}{\\partial W_1}\n",
    "\\end{align}\n",
    "\n",
    "Now we can gather the different partial derivatives we need:\n",
    "\\begin{align}\n",
    "\\frac{\\partial L}{\\partial \\hat{y}} &= - (y - \\hat{y}) &\\in \\mathbb{R}\\\\\n",
    "\\frac{\\partial \\hat{y}}{\\partial z_2} & = \\sigma(z_2) \\cdot (1-\\sigma(z_2)) &\\in \\mathbb{R}^{1 \\times 1}\\\\\n",
    "\\frac{\\partial h}{\\partial z_1} & = \\sigma(z_1) \\cdot (1-\\sigma(z_1)) &\\in \\mathbb{R}^{5 \\times 1} \\\\\n",
    "\\frac{\\partial z_2}{\\partial W_2} &= h^T &\\in \\mathbb{R}^{1 \\times 5} \\\\\n",
    "\\frac{\\partial z_2}{\\partial h} &= W_2^T &\\in \\mathbb{R}^{1 \\times 5}\n",
    "\\end{align}\n",
    "\n",
    "The last derivative $\\frac{\\partial z_1}{\\partial W_1}$ is a bit more difficult. $z_1$ is column vector and $W_1$ is a matrix. In this case we need a generalized Jacobian, which looks like this:\n",
    "\\begin{align}\n",
    "\\left(\\frac{\\partial z_1}{\\partial W_1}\\right)_{i,j,k}  &= \\frac{\\partial z_{1;i}}{\\partial W_{1;j,k}} &\\in \\mathbb{R}^{5 \\times 2 \\times 5}\\\\\n",
    "\\text{e.g.} \\frac{\\partial z_{1;1}}{\\partial W_{1;2,1}} &= x_2 \\text{ and } \\frac{\\partial z_{1;1}}{\\partial W_{1;2,2}} = 0\n",
    "\\end{align}\n",
    "\n",
    "For more infos on derivatives, take a look at [this document](https://cs231n.stanford.edu/handouts/derivatives.pdf).\n",
    "\n",
    "Now we only need to put the partial derivatives together and multiply them correctly. Note that $\\frac{\\partial h}{\\partial z_1}$ is done element-wise and therefore needs to be multiplied accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Implement the training loop over the dataset\n",
    "\n",
    "The training loop should consist of:\n",
    "* a forward pass, computing the output of the net.\n",
    "* computing the loss\n",
    "* a backward pass, computing the gradients of the weights w.r.t. the loss\n",
    "* updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(x, y, w_1, w_2):\n",
    "    \"\"\"\n",
    "    The training loop.\n",
    "    \n",
    "    Input values:\n",
    "        x : a tensor of all input data points\n",
    "        y : a tensor of all labels\n",
    "        w_1, w_2 : the weight matrices\n",
    "    \n",
    "    Output values:\n",
    "        w_1, w_2 : the updated weight matrices\n",
    "        cum_loss : the cummulated loss over all data points\n",
    "    \"\"\"\n",
    "    #ToDo: Implement the training loop\n",
    "    \n",
    "    return w_1, w_2, cum_loss\n",
    "\n",
    "w_1, w_2, cum_loss = train_one_epoch(X.reshape((4, 2, 1)), y, w_1, w_2, 1)\n",
    "print(cum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXP9IsW5LHKX",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(x, y, w_1, w_2, lr):\n",
    "     \"\"\"\n",
    "    The training loop.\n",
    "    \n",
    "    Input values:\n",
    "        x : a tensor of all input data points\n",
    "        y : a tensor of all labels\n",
    "        w_1, w_2 : the weight matrices\n",
    "        lr : the learning rate\n",
    "    \n",
    "    Output values:\n",
    "        w_1, w_2 : the updated weight matrices\n",
    "        cum_loss : the cummulated loss over all data points\n",
    "    \"\"\"\n",
    "    cum_loss = 0\n",
    "    for x_i, y_i in zip(x, y):\n",
    "        yhat, z_1, h, z_2 = forward(x_i, w_1, w_2)\n",
    "        loss = sq_loss(yhat, y_i)\n",
    "        w1, w2 = backward(x_i, y_i, yhat, w_1, w_2, z_1, h, z_2, lr)\n",
    "        cum_loss += loss.item()\n",
    "    cum_loss = cum_loss / len(y)\n",
    "    return w1, w2, cum_loss\n",
    "\n",
    "w_1, w_2, cum_loss = train_one_epoch(X.reshape((4, 2, 1)), y, w_1, w_2, 1)\n",
    "print(cum_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Train the model\n",
    "\n",
    "Run the training loop for 1001 epochs. Afterwards, test your model by printing its predictions on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Train your model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train(x, y, num_epochs):\n",
    "    \"\"\"\n",
    "    Model training\n",
    "    \n",
    "    Input values:\n",
    "        x : a tensor of all input data points\n",
    "        y : a tensor of all labels\n",
    "        num_epochs : number of epochs to train\n",
    "        \n",
    "    Output values:\n",
    "        w_1, w_2 : the weight matrices after training.\n",
    "    \"\"\"\n",
    "    w_1, w_2 = initialize_weights(g)\n",
    "    x = x.reshape(4, -1, 1)\n",
    "    for i in range(num_epochs):\n",
    "        w_1, w_2, cum_loss = train_one_epoch(x, y, w_1, w_2)\n",
    "        if i%100==0:\n",
    "            print(f\"Epoch {i} \\t ==> \\t Loss {cum_loss:.8f}\")\n",
    "    return w_1, w_2\n",
    "\n",
    "# Training\n",
    "w_1, w_2 = train(X, y, 1001)\n",
    "\n",
    "# Testing\n",
    "for x_i, y_i in zip(X,y):\n",
    "    yhat, z1, z2, h = forward(x_i, w_1, w_2)\n",
    "    print(f\"{np.array(x_i)}\\t{y_i}\\t{yhat.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3_Neural_Networks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl_lai)",
   "language": "python",
   "name": "dl_lai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "288px",
    "width": "597px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "339px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
