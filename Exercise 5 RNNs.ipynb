{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saItxTU2Vs9p"
   },
   "source": [
    "# Deep Learning\n",
    "## Exercise 5 - Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQR03x-fOzKe"
   },
   "source": [
    "### 0. LSTMs in PyTorch\n",
    "We recommend reading [this tutorial](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) on building an LSTM model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdYYZ8CbD7WL"
   },
   "source": [
    "### 1. Word2Vec\n",
    "The [MovieLens 25M](https://grouplens.org/datasets/movielens/25m/) dataset contains movie titles and corresponding tags added by users. For every movie in the dataset, we concatenate all tags and treat the resulting list of tags as a sentence.\n",
    "\n",
    "Your task is to build a simple search engine based on Word2Vec, treating each tag as a word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BpqaFRHQD7WN"
   },
   "source": [
    "Run the cells below to download, extract and setup the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jc7nwTB1OzKg",
    "outputId": "a719248c-0faf-4f62-8c32-432fe3a82622",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget -P './data' 'http://files.grouplens.org/datasets/movielens/ml-25m.zip' && unzip -o './data/ml-25m.zip' -d './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAEOo_JVOzKg",
    "outputId": "5b6b7f01-d5dd-44ce-edd7-b8306eb2a0bf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load and preprocess data\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "data_dir = './data/ml-25m'\n",
    "movies_df = pd.read_csv(data_dir + '/movies.csv')\n",
    "tags_df = pd.read_csv(data_dir + '/tags.csv', converters={'tag': str}).groupby('movieId')['tag'].agg(list)\n",
    "df = pd.merge(movies_df[['movieId', 'title']], tags_df, how='right', left_on='movieId', right_index=True).rename(columns={'tag': 'tags'})\n",
    "df = df.drop('movieId', axis=1)\n",
    "df['movie_id'] = list(range(len(df)))\n",
    "df = df.set_index('movie_id', drop=False)\n",
    "\n",
    "print(f'number of movies in dataset: {df.shape[0]}')\n",
    "print(f'first movie: title: {df.iloc[0][\"title\"]}')\n",
    "print(f'first movie: first 20 tags: {df.iloc[0][\"tags\"][:20]}...')\n",
    "print(f'id of title \"Toy Story (1995)\": {df.loc[df.loc[:,\"title\"]==\"Toy Story (1995)\", \"movie_id\"].item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been loaded into a pandas DataFrame, which now has the columns: `movie_id`, `title`, `tags`. It also has an index, which is identical to the column `movie_id`.\n",
    "\n",
    "The cleanest way to access the data is using `.loc` or `.iloc` (make sure you understand the difference).\n",
    "\n",
    "In the following tasks, exploit the data structure by avoiding any loops over the entries. *Hint*: check out `df.map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train a Word2Vec model\n",
    "\n",
    "Use the [gensim's Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) function to train on the tag sentences and optain $64$-dimensional word embeddings. Set the window size to $5$ and the min_count to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gensim.models import Word2Vec # you can ignore any UserWarning about the missing levenshtein module\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: build your Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TGdkAiIOOzKh",
    "outputId": "f0a3a5b8-a673-43f0-9813-f667c5296cee",
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences = df['tags'], vector_size=64, window = 5, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We only need the word vectors for all further tasks, so we don't need to keep the model.\n",
    "#You can access the representation of 'word' by word_vectors['word']\n",
    "word_vectors = model.wv\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a vector representation for each movie\n",
    "\n",
    "We want to represent a movie $m$ that has a set of tags $T$ as the average of all word vectors, i.e.\n",
    "\\begin{equation}\n",
    "    v_m = \\frac{\\sum_{t \\in T} E(t)}{|T|}\n",
    "\\end{equation}\n",
    "where $E(t)$ is the embedding of a tag $t$ and $v_m$ is the vector representation of $m$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Extract vector representations for each movie.\n",
    "# Don't use a for-loop. Exploit the functionalities of the pandas.DataFrame()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def get_representation(tags_list, word_vectors):\n",
    "    \"\"\"\n",
    "    Transform the list of tags into a representation for the corresponding movie.\n",
    "    \n",
    "    Input values:\n",
    "        tags_list : a list of tags\n",
    "        word_vectors : the KeyedVectors from your Word2Vec model.\n",
    "    \n",
    "    Output value:\n",
    "        representation : the representation\n",
    "    \"\"\"\n",
    "    vectors_list=[word_vectors[tag] for tag in tags_list]\n",
    "    vectors = np.array(vectors_list)\n",
    "    representation = np.mean(vectors, axis=0)\n",
    "    return representation\n",
    "\n",
    "print(get_representation(df.loc[0,'tags'], word_vectors)) \n",
    "df['representations'] = df['tags'].map(lambda tags: get_representation(tags, word_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Implement a small search engine for the query `Toy Story (1995)`\n",
    "\n",
    "For movies and queries, we use the representation defined above. The relevance of a movie w.r.t. a query should be the cosine similarity between the two vectors.\n",
    "\n",
    "Print the top-$10$ results (the movie titles) for the query title `Toy Story (1995)`.\n",
    "\n",
    "*Hint*: the most relevant movie to a query is the query movie itself. It should have a cosine similarity of 1.0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "q = \"Toy Story (1995)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Find the 10 most similar movies to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "query = get_representation(df.loc[df.loc[:,\"title\"]==q, \"tags\"].item(), word_vectors)\n",
    "df['cosine_sim'] = df['representations'].map(\n",
    "    lambda rep: cosine_similarity(torch.tensor(rep), torch.tensor(query), dim=0).item())\n",
    "df = df.sort_values(by='cosine_sim', ascending=False)\n",
    "print(df.iloc[:10][['title', 'cosine_sim']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tl92kAaNVs9s"
   },
   "source": [
    "### 2. Sentiment Classification with LSTMs\n",
    "This task is about implementing a many-to-one LSTM for sentiment classification. We will use the IMDB dataset, which contains movie reviews associated with sentiments (positive/negative). The task is to classify each review into one of these two classes.\n",
    "\n",
    "We provide you with the following data setup:\n",
    "First, we load the IMDB dataset from `torchtext.datasets`, build the vocabulary and split to train/valid/test instances. Don't worry if you get confused by the dataset API, lets focus on the model architecture and training methods.\n",
    "\n",
    "For development and debugging you can set `debugging = True` which will load a smaller subset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-IlyXicOzKi",
    "outputId": "c024f113-769e-40ca-d95d-8dd02bdeb6cf"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data, datasets, vocab\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_seed = 0\n",
    "data_directory = './data'\n",
    "debugging = True #This can be set to True, if you want to test your implementation on a smaller subset\n",
    "\n",
    "\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "max_length = 200   # we want the maximum words in each text instance to be 200.\n",
    "max_vocab = 20000  # We want the vocabulary size not to exceed 20000.\n",
    "\n",
    "# define a function to preprocess and tokenize raw text input\n",
    "tokenizer = data.get_tokenizer('basic_english')\n",
    "def text_tokenizer(entry):\n",
    "    entry = re.sub('<\\w{1,2} />', ' ', entry) #replace <br /> and similar\n",
    "    entry = re.sub(r'[^\\w\\s]', ' ', entry) #remove any non-space or non-word characters\n",
    "    entry = re.sub(r'\\s+', ' ', entry) #replace multiple spaces by one space\n",
    "    tokens = tokenizer(entry)\n",
    "    return tokens\n",
    "\n",
    "# read the dataset, the first call also downloads the dataset. Split the training_data into training and validation\n",
    "train_set, test_set = datasets.IMDB(root=data_directory)\n",
    "train_set = list(train_set)\n",
    "test_set = list(test_set)\n",
    "\n",
    "\n",
    "if debugging: \n",
    "    train_labels = [l for l,t in train_set]\n",
    "    test_labels = [l for l, t in test_set]\n",
    "    train_set, _ = train_test_split(train_set, train_size=0.2, stratify=train_labels, random_state=random_seed)\n",
    "    test_set, _ = train_test_split(test_set, train_size=0.2, stratify=test_labels, random_state=random_seed)\n",
    "\n",
    "train_labels = [l for l,t in train_set]\n",
    "train_set, val_set = train_test_split(train_set, train_size=0.7, stratify=train_labels, random_state=random_seed)\n",
    "\n",
    "# build the vocabulary from the training data\n",
    "counter = Counter()\n",
    "for label, text in train_set:\n",
    "    tokens = text_tokenizer(text)\n",
    "    counter.update(tokens)\n",
    "    \n",
    "vocabulary = vocab.vocab(OrderedDict(counter.most_common()[:max_vocab]))\n",
    "special_tokens = ['<unk>', '<pad>']\n",
    "for i, tok in enumerate(special_tokens):\n",
    "    vocabulary.insert_token(tok, i)\n",
    "vocabulary.set_default_index(vocabulary['<unk>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CgXYLMWbOzKj"
   },
   "source": [
    "You can get the text vocabulary size with `len(vocabulary)`. Note that two special tokens `('<pad>', '<unk>')` are added to the vocabulary. You can check the index by e.g, `vocabulary.get_stoi()['<pad>']` or simply `vocabulary['<pad>']` and the inverse index by `vocabulary.get_itos()[1]`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p53YEJPTOzKj"
   },
   "source": [
    "Next, we make the dataset iterable (in batches) for training the model. We need to decide on the batch size. Note that each batch of data contains a tuple of input text and output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXuOaZRSOzKj"
   },
   "outputs": [],
   "source": [
    "# build the train, validation and test dataloaders\n",
    "def collate_fn(batch):\n",
    "    labels, indexes = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(1 if label =='pos' else 0)\n",
    "        \n",
    "        tokens = text_tokenizer(text)\n",
    "        indexes += [torch.tensor([vocabulary[t] for t in tokens][:max_length])]\n",
    "    labels = torch.tensor(labels)\n",
    "    padded_indices = pad_sequence(indexes, padding_value=vocabulary['<pad>'], batch_first=True)\n",
    "        \n",
    "    return labels, padded_indices\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, \n",
    "                              collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True, \n",
    "                              collate_fn=collate_fn, drop_last=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=32, shuffle=False, \n",
    "                              collate_fn=collate_fn, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rEc-OMh2OzKk",
    "outputId": "164ae339-29ed-43ef-8c5f-672c82cc890b"
   },
   "outputs": [],
   "source": [
    "# example of iterating the training data\n",
    "for label, indices in train_dataloader:\n",
    "    print(indices) # tensor of size (batch_size x max_length) containing the tokenized words\n",
    "    print(label) # 1d tensor containing the binary labels\n",
    "    print(f'first sentence in batch:\\n{\" \".join(vocabulary.lookup_tokens(indices.squeeze()[0].numpy().tolist()))}')\n",
    "    print(f'first label in batch: {\"pos\" if label[0]==1 else \"neg\"}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwXq5cIdOzKk"
   },
   "source": [
    "Now, the datasets have been prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a model named SentimentClassifier with following features:\n",
    "* An Embedding layer with 100 embedding dimension. Use `nn.Embedding`.\n",
    "* One bidirectional LSTM layer with hidden dimension 400. Use `nn.LSTM` and set `bidirectional=True`.\n",
    "* Some dropout with probability 0.3 on the LSTM output. Use `nn.Dropout`.\n",
    "* One fully connected layer to map the output features of the LSTM layer to a single output. Use a sigmoid activation for the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# todo: fill the __init__() and forward() function. Add arguments to both if you need them.\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, ):\n",
    "        \n",
    "    \n",
    "SC = SentimentClassifier()\n",
    "\n",
    "for label, indices in train_dataloader:\n",
    "    print(indices[0])\n",
    "    print(SC(indices))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDGlDwRYOzKl",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=100)\n",
    "        self.lstm = nn.LSTM(input_size=100, hidden_size=200, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.full_conn = nn.Linear(in_features=400, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        h = self.emb(text)\n",
    "        h,_ = self.lstm(h)\n",
    "        h = h[:, -1, :]\n",
    "        h = self.dropout(h)\n",
    "        h = self.full_conn(h)\n",
    "        res = self.sigmoid(h)\n",
    "        return res.flatten()\n",
    "    \n",
    "SC = SentimentClassifier(len(vocabulary))\n",
    "\n",
    "for label, indices in train_dataloader:\n",
    "    print(indices[0])\n",
    "    print(SC(indices))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Train your model\n",
    "\n",
    "Use the binary cross entropy loss (`torch.nn.BCELoss`) and the adam optimizer (use `torch.optim.Adam` with default parameters) for a maximum of 20 epochs. After every epoch, compute the validation loss. Implement an early stopping mechanism that keeps track of the best model parameters based on lowest validation loss. Stop training if the validation loss does not improve for continuous 3 epochs and revert back to the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implement training your model with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(SC.parameters())\n",
    "\n",
    "def train(num_epochs, model, loss_funtion, optimizer, train_loader, val_loader, break_criterium):\n",
    "    best_val_loss = 100000\n",
    "    no_improve=0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for labels, texts in tqdm(train_loader, desc='Train Iter', ascii=True):\n",
    "            output = model(texts)\n",
    "            loss = loss_function(output, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            cum_loss = 0\n",
    "            for labels, texts in tqdm(val_loader, desc='Valid Iter', ascii=True):\n",
    "                output = model(texts)\n",
    "                loss = loss_function(output, labels.float())\n",
    "                cum_loss += loss.item()\n",
    "            cum_loss = cum_loss/len(val_loader)\n",
    "            print(f\"Epoch {epoch} \\t Train Loss {train_loss:.5f} \\t Val Loss {cum_loss:.5f}\")\n",
    "            if cum_loss < best_val_loss:\n",
    "                best_val_loss = cum_loss\n",
    "                torch.save(model.state_dict(), 'imdb.pt')   # save the best model\n",
    "                no_improve=0\n",
    "            else:\n",
    "                no_improve+=1\n",
    "            if no_improve >= break_criterium:\n",
    "                break\n",
    "    model.load_state_dict(torch.load('imdb.pt'))\n",
    "    return model\n",
    "    \n",
    "SC = train(20, SC, loss_function, optimizer, train_dataloader, val_dataloader, 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Evaluate the accuracy of your trained model on the `test_iter` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implement the evaluation of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_entries = 0\n",
    "    for labels, texts in tqdm(test_loader, desc='Test Iter', ascii=True):\n",
    "        output = model(texts)\n",
    "        preds = (output>0.5).int()\n",
    "        correct += (preds == labels).sum()\n",
    "        total_entries += len(texts)\n",
    "    return (correct/total_entries).item()\n",
    "    \n",
    "acc = evaluate(SC, test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:dl_lai]",
   "language": "python",
   "name": "conda-env-dl_lai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
