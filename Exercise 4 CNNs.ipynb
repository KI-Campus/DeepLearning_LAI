{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoXOcYbF52Zf"
   },
   "source": [
    "# Deep Learning\n",
    "## Exercise 4 - Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6mrWKe0Q_xr"
   },
   "source": [
    "### 0. Neural Networks with PyTorch\n",
    "Continuing from the last exercise, work through the **What is torch.nn really?** [tutorial notebook](https://pytorch.org/tutorials/beginner/nn_tutorial.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yliduHUQ_xr"
   },
   "source": [
    "### 1. CNN Forward Pass\n",
    "\n",
    "In this exercise we address a convolutional neural network (CNN) with one-dimensional input, as we also did in the exercise *CNNs Forward Pass*, just this time we will do it with `torch` instead of manually.\n",
    "\n",
    "While two-dimensional CNNs can be used for example for grayscale images, one-dimensional CNNs could be used for time-series such as temperature or humidity readings. Concepts for the 1D-case are equivalent to 2D networks. We interpret data in our network as three-dimensional arrays where a row denotes a feature map, a column denotes a single dimension of the observation, and the depth of the array represents different observations. As we will only work with a single input vector, the depth will always be one.\n",
    "\n",
    "In the following steps you will build a CNN for following input:\n",
    "* Input $I$: Matrix of size $1 \\times 1 \\times 12$. We therefore have one input consisting of a single feature map with twelve dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Define the input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0], dtype=torch.float).unsqueeze(0).unsqueeze(0)\n",
    "print(f'Input:\\n{x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define the layers of your CNN using `torch.nn`\n",
    "\n",
    "The CNN consists of following layers:\n",
    "1. Convolutional layer with kernel of size $1 \\times 3 \\times 2$.\n",
    "2. Max-pooling layer with stride $2$ and filter size $2$. Note that max-pooling pools each feature map separately.\n",
    "3. Convolutional layer with convolutional kernel of size $2 \\times 3 \\times 1$.\n",
    "4. Fully connected layer that maps all of its inputs to two outputs.\n",
    "5. A final sigmoid activation function\n",
    "\n",
    "Omit all bias terms and use no padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Define all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "conv_1 = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=3, bias=False, padding=0)\n",
    "max_pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "conv_2 = nn.Conv1d(in_channels=2, out_channels=1, kernel_size=3, bias=False, padding=0)\n",
    "full_connected = nn.Linear(in_features=3, out_features=2, bias=False)\n",
    "activation = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Manually set the weights.\n",
    "\n",
    "To assign new weights, you have to wrap a tensor in `nn.Parameter()`.\n",
    "The layers have following weights:\n",
    "* The first convolutional layer has filters: $F_0^1 = (-1, 0, 1)$ and $F_1^1 = (1, 0, -1)$\n",
    "* The second convolutional layer has the filter: $F_0^2 = ((-1, 0, 1), (1, 0, -1))$.\n",
    "* The first output of the fully connected layer is calculated as the negative sum of all its inputs, and the second output is calculated as the positive sum of all its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Set all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "conv_1.weight = nn.Parameter(torch.tensor([[[-1,0,1]],[[1,0,-1]]], dtype=torch.float))\n",
    "conv_2.weight = nn.Parameter(torch.tensor([[[-1,0,1],[1,0,-1]]], dtype=torch.float))\n",
    "full_connected.weight = nn.Parameter(torch.tensor([[-1, -1, -1], [1, 1, 1]], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. perform a forward pass, printing the output after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implement a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQtsbmB4Q_xr",
    "outputId": "0239240f-0bf1-4521-8653-4e5520db402c",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "h1 = conv_1(x)\n",
    "print(h1)\n",
    "h2 = max_pool(h1)\n",
    "print(h2)\n",
    "h3 = conv_2(h2)\n",
    "print(h3)\n",
    "h4 = full_connected(h3)\n",
    "print(h4)\n",
    "output = activation(h4)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQzmwpkFQ_xs"
   },
   "source": [
    "### 2. Training an Image Classifier\n",
    "The CIFAR 10 dataset is an image dataset containing images from 10 classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. Your task is to train a CNN to classify given input images into one of these 10 classes.\n",
    "\n",
    "\n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size. More information on CIFAR 10 can be found over [here](https://www.cs.toronto.edu/~kriz/cifar.html). This means we have to create a CNN with 2D convolutions and 3 input channels.\n",
    "\n",
    "The dataset is already loaded for you. For development and debugging, you can set `use_subset = True` to work with a small subset of the data (100 examples). For your final training run, set it back to `False`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189,
     "referenced_widgets": [
      "505f4a7ec51b49388b1e91708fbfc0f9",
      "53bda009508f42d892f923b9ca027dea",
      "5d552e69de4c4488b6c239e3d3f30890",
      "39ec1f26c2014859bf0c47949e8b2cc0",
      "fd748a4da21640d3b4af36740710750b",
      "37572bbcdaa441fca17b7710c2de4643",
      "0b60778232db4527ba973ece63bebf07",
      "945745f97b5b4f98964ee6408be5810b"
     ]
    },
    "id": "ERcyYutrQ_xt",
    "outputId": "66e77ac1-245a-4623-ea64-653a722cf2ae"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "use_subset = False # Set this to True for debugging purposes.\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "classes = train_dataset.classes\n",
    "\n",
    "if use_subset:\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 100))\n",
    "    val_dataset = torch.utils.data.Subset(val_dataset, torch.arange(0, 100))\n",
    "\n",
    "print(f'classes: {classes}\\nnumber of instances:\\n\\ttrain: {len(train_dataset)}\\n\\tval: {len(val_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dOzoYXEQ_xu"
   },
   "source": [
    "Visualizing a few examples. Note that the images are only 32x32 pixels, so they look quite pixelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "L2Bx_ur5Q_xu",
    "outputId": "8cb0c6f0-917b-4fc9-9562-7d72d0d33a33",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_examples(n):\n",
    "    for i in range(n):\n",
    "        index = torch.randint(0, len(train_dataset), size=(1,)) # select a random example\n",
    "        image, target = train_dataset[index]\n",
    "        print(f'image of shape: {image.shape}')\n",
    "        print(f'label: {classes[target]}')\n",
    "        plt.imshow(image.permute(1,2,0).numpy())\n",
    "        plt.show()\n",
    "\n",
    "show_examples(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  Create a training and validation dataloader.\n",
    "\n",
    "Use `torch.utils.data.DataLoader` to retrieve batches of data. Use a batch size of 32. Make sure to have the training set shuffled after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: create a dataloader for the trainin and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  Define a CNN using `torch.nn`.\n",
    "\n",
    "You can use `nn.Sequential` as a container for your layers or define your own `nn.Module`. The network should have the following architecture:\n",
    "\n",
    "1. A 2-d convolutional layer with 48 output channels, a kernel size of (3x3) and (1x1) padding.\n",
    "2. A second 2-d convolutional layer with 96 output channels, a kernel size of (3x3) and (1x1) padding.\n",
    "3. A Max-Pooling layer with a 2x2 kernel.\n",
    "4. A third 2-d convolutional layer with 192 output channels, a kernel size of (3x3) and (1x1) padding.\n",
    "5. A Max-Pooling layer with a 2x2 kernel.\n",
    "6. A fully connected layer with output dimension of 64.\n",
    "7. A final, fully connected classification layer, with output dimension of 10 (one for each class).\n",
    "\n",
    "Directly after every conv and fully connected layer, apply a ReLU non-linearity. Do not apply a non-linearity after the final layer.\n",
    "    \n",
    "Remember to flatten your hidden representations before the first fully connected layer. Hint: `nn.Flatten`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Define the CNN\n",
    "\n",
    "\n",
    "#Test on one Batch:\n",
    "image_batch, target_batch= iter(train_loader).next()\n",
    "print(model(image_batch).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IM1V4OKLQ_xv",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(3, 48, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 96, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(96, 192, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(192*8*8 ,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "            )\n",
    "\n",
    "#Test on one Batch:\n",
    "image_batch, target_batch= iter(train_loader).next()\n",
    "print(model(image_batch).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  Define loss function and optimizer.\n",
    "\n",
    "Use cross entropy loss and SGD as the optimizer. Use a learning rate of 0.01 and set the momentum parameter of `torch.optim.SGD` to 0.9 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Define loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  Train the CNN for 5 epochs and validate your trained model by computing the accuracy on the validation dataset.\n",
    "\n",
    "\n",
    "To accelerate the training, you can use a GPU.\n",
    "\n",
    "*Hint:* If you want to monitor the progress of training, take a look at `tqdm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implement Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train(epochs, model, loss_function, opt, train_loader):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        cum_loss = 0\n",
    "        num_batches = 0\n",
    "        for img, label in tqdm(train_loader, desc='Train Iteration',ascii=True):\n",
    "            output = model(img)\n",
    "            loss = loss_function(output, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            cum_loss += loss.item()\n",
    "            num_batches +=1\n",
    "        print(f\"Epoch {epoch} \\t ----> \\t Loss {cum_loss/len(train_loader) :.5f}\")\n",
    "              \n",
    "train(5, model, loss, optimizer, train_loader)\n",
    "\n",
    "def eval(model, loss_funct, val_loader):\n",
    "    model.eval()\n",
    "    total_matches = 0\n",
    "    val_entries = 0\n",
    "    cum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(val_loader, desc='Val Iteration', ascii=True):\n",
    "            output = model(img)\n",
    "            loss = loss_funct(output, label)\n",
    "            cum_loss += loss.item()\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            num_matches_batch = (prediction==label).sum()\n",
    "            total_matches += num_matches_batch\n",
    "            val_entries += len(img)\n",
    "    accuracy = total_matches/val_entries\n",
    "    loss = cum_loss/len(val_loader)\n",
    "    return accuracy.item(), loss\n",
    "\n",
    "acc, val_loss = eval(model, loss, val_loader)\n",
    "print(f\"Validation Accuracy: {acc:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "4_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:dl_lai]",
   "language": "python",
   "name": "conda-env-dl_lai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "metadata": {
   "interpreter": {
    "hash": "327b9f1a283d8efdfbc5d3bf3318e496a7cbefa5866e024fe1786ab8e84cd9e3"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b60778232db4527ba973ece63bebf07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37572bbcdaa441fca17b7710c2de4643": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ec1f26c2014859bf0c47949e8b2cc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_945745f97b5b4f98964ee6408be5810b",
      "placeholder": "​",
      "style": "IPY_MODEL_0b60778232db4527ba973ece63bebf07",
      "value": " 170499072/? [00:21&lt;00:00, 8097129.97it/s]"
     }
    },
    "505f4a7ec51b49388b1e91708fbfc0f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5d552e69de4c4488b6c239e3d3f30890",
       "IPY_MODEL_39ec1f26c2014859bf0c47949e8b2cc0"
      ],
      "layout": "IPY_MODEL_53bda009508f42d892f923b9ca027dea"
     }
    },
    "53bda009508f42d892f923b9ca027dea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d552e69de4c4488b6c239e3d3f30890": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37572bbcdaa441fca17b7710c2de4643",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fd748a4da21640d3b4af36740710750b",
      "value": 170498071
     }
    },
    "945745f97b5b4f98964ee6408be5810b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fd748a4da21640d3b4af36740710750b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
