{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saItxTU2Vs9p"
   },
   "source": [
    "# Deep Learning\n",
    "## Exercise 7 - Regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLi_sssK3rKm"
   },
   "source": [
    "### Effect of Regularization\n",
    "\n",
    "Below you find the data setup for a binary classification problem in 2D input space. We will train two neural networks to gain some inuition about the effect of regularization. In particular, you should train the same model twice, while only adding L2 regularization to one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "EpS5YnfcgyAz",
    "outputId": "9102c7ac-4e82-4696-9d3f-62d0c8493a05",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "\n",
    "def generate_data(center, spread, samples, test=False):\n",
    "    x0 = torch.empty(samples, 1).uniform_(center[0] - spread, center[0] + spread)\n",
    "    x1 = torch.empty(samples, 1).uniform_(center[1] - spread, center[1] + spread)\n",
    "    y0 = torch.zeros(samples)\n",
    "    if test: \n",
    "        f = lambda point: ((center[0]) + -point[0]) + ((center[1] + spread) + -point[1])\n",
    "    else:\n",
    "        f = lambda point: ((center[0]) + -point[0]) + ((center[1] + spread) + -point[1]) + 5 * torch.cos(point[0] * 10)\n",
    "    y0 = torch.where(f((x0, x1)) > 5, 0, 1).squeeze()\n",
    "    # print(f(x0))\n",
    "\n",
    "    return torch.cat([x0, x1], dim=1), y0\n",
    "\n",
    "\n",
    "X_train, y_train = generate_data(center=(10,10), spread=5, samples=500)\n",
    "X_test, y_test = generate_data(center=(10,10), spread=5, samples=50, test=True)\n",
    "\n",
    "\n",
    "def visualize(model=None):\n",
    "    if model is not None:\n",
    "        vis_points = torch.meshgrid(torch.arange(5, 15, 0.1), torch.arange(5, 15, 0.1))\n",
    "        vis_points = torch.stack([vis_points[0].reshape(-1), vis_points[1].reshape(-1)], dim=1)\n",
    "        preds = model(vis_points)\n",
    "        preds = torch.where(preds > 0.5, 1, 0)\n",
    "        plt.scatter(vis_points[:, 0], vis_points[:, 1], s=10, alpha=0.1, c=['orange' if target == 1 else 'blue' for target in preds])\n",
    "    print('Training data:')\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], marker='.', s=100, c=['orange' if target == 1 else 'blue' for target in y_train])\n",
    "    plt.show()\n",
    "\n",
    "    print('Test data:')\n",
    "    if model is not None:\n",
    "        plt.scatter(vis_points[:, 0], vis_points[:, 1], s=10, alpha=0.1, c=['orange' if target == 1 else 'blue' for target in preds])\n",
    "    plt.scatter(X_test[:, 0], X_test[:, 1], marker='.', s=100, c=['orange' if target == 1 else 'blue' for target in y_test])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize()\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Set up the model\n",
    "\n",
    "Use the following network architecture:\n",
    "- 3 fully connected hidden layers consisting of 64 neurons each.\n",
    "- A ReLU activation after every layer except the output layer\n",
    "- A fully connected output layer, mapping its 64 inputs to a single output neuron and using a Sigmoid activation to push values between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = nn.Sequential(        \n",
    "        nn.Linear(in_features=2, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(in_features=64, out_features=1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Training  without regularization.\n",
    "Train the network using the following hyperparameters:\n",
    "- Train for 1000 epochs.\n",
    "- Use the Adam optimizer with a learning rate of 0.001.\n",
    "- Use a batch size of 32.\n",
    "- Use a binary cross entropy loss.\n",
    "\n",
    "Compute the accuracy on both training and test set. Use a threshold of 0.5 to binarize the model's output. How does your training and testing accuracy evolve?\n",
    "\n",
    "Visualize your model's decision boundary using the given `visualize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Set up the training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, opt, train_dl, test_dl):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    loss_function = nn.BCELoss()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        cum_loss = 0\n",
    "        total_matches = 0\n",
    "        train_entries = 0\n",
    "        for points, labels in train_dl:\n",
    "            output = model(points).flatten()\n",
    "            loss = loss_function(output, labels.float())\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            cum_loss += loss.item()\n",
    "            \n",
    "            total_matches += accuracy(output, labels)\n",
    "            train_entries += len(points)\n",
    "        train_loss += [cum_loss/len(train_dl)]\n",
    "        train_acc += [total_matches/train_entries]\n",
    "        \n",
    "        acc, test_loss = evaluation(model, loss_function, test_dl)\n",
    "        val_loss += [test_loss]\n",
    "        val_acc += [acc]\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            print(f\"Epoch {epoch} \\t ----> \\t Loss {cum_loss/len(train_dl) :.5f} \\t ----- \\t Val Loss {test_loss:.5f} \\t ----- \\t Accuracy {acc:.5f}\")\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def accuracy(output, labels):\n",
    "    prediction = (output>0.5).int()\n",
    "    num_matches = (prediction==labels).sum()\n",
    "    return num_matches\n",
    "    \n",
    "        \n",
    "def evaluation(model, loss_funct, val_dl):\n",
    "    model.eval()\n",
    "    total_matches = 0\n",
    "    val_entries = 0\n",
    "    cum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for points, labels in val_dl:\n",
    "            output = model(points).flatten()\n",
    "            loss = loss_funct(output, labels.float())\n",
    "            cum_loss += loss.item()\n",
    "\n",
    "            total_matches += accuracy(output, labels)\n",
    "            val_entries += len(points)\n",
    "    acc = total_matches/val_entries\n",
    "    loss = cum_loss/len(val_dl)\n",
    "    return acc.item(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dl = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(ds_test, batch_size=32)\n",
    "num_epochs = 1000\n",
    "\n",
    "model = get_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loss, train_acc, val_loss, val_acc = train(num_epochs, model, optimizer, train_dl, test_dl)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "ax1.plot(list(range(num_epochs)),train_loss, label='Train')\n",
    "ax1.plot(list(range(num_epochs)), val_loss, label='Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(list(range(num_epochs)),train_acc, label='Train')\n",
    "ax2.plot(list(range(num_epochs)), val_acc, label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "visualize(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Training with Regularization\n",
    "Train the same model with the same settings again, only this time add an L2 regularization penalty of 0.01 to the model (Hint: weight decay in the optimizer).\n",
    "\n",
    "Again, visualize your model's decision boundary using the given `visualize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run training with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "train_dl = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "test_dl = torch.utils.data.DataLoader(ds_test, batch_size=32)\n",
    "num_epochs = 1000\n",
    "\n",
    "model = get_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "train_loss, train_acc, val_loss, val_acc = train(num_epochs, model, optimizer, train_dl, test_dl)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "ax1.plot(list(range(num_epochs)),train_loss, label='Train')\n",
    "ax1.plot(list(range(num_epochs)), val_loss, label='Test')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(list(range(num_epochs)),train_acc, label='Train')\n",
    "ax2.plot(list(range(num_epochs)), val_acc, label='Test')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "plt.show()\n",
    "\n",
    "visualize(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7_Regularization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python DL-LAI",
   "language": "python",
   "name": "python-dl_lai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
