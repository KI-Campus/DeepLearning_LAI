{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Excercise 11: Graph Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will explore graph neural networks. We will lean heavily on the [tutorials](https://pytorch-geometric.readthedocs.io/en/latest/get_started/colabs.html) provided by `pytorch-geometric`. Take a look at those tutorials for more explanations and exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F1op-CbyLuN4",
    "outputId": "d8fae9ee-6f6d-4e6e-9e73-2e4d98f33292"
   },
   "outputs": [],
   "source": [
    "# Helper function for visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2, init='pca', learning_rate='auto').fit_transform(h.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dszt2RUHE7lW"
   },
   "source": [
    "### 1. Node Classification\n",
    "\n",
    "We want to build a Graph Neural Network for node classification. \n",
    "\n",
    "The data reading is already implemented for you. We will use (as the official [tutorial](https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX?usp=sharing)) the `Cora` dataset. Each node represents a document, which is represented by a bag-of-words feature vector. Two documents are connected if there exists a citation link between them. Each document can be classified in one of 7 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imGrKO5YH11-",
    "outputId": "373bf1d0-9ad2-469a-caf5-ebed4f4743a3"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='./data/Planetoid', name='Cora', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqWR0j_kIx67"
   },
   "source": [
    "Note that the training dataset is quite small (only ~5%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build a Graph Neural Network\n",
    "\n",
    "Your network should consist of:\n",
    "- a graph convolutional layer `torch_geometric.nn.GCNConv`, which takes the bag-of-words feature vectors and the edges as input and returns a 16-dim output.\n",
    "- a ReLU layer\n",
    "- a dropout layer with dropout probability of 0.3\n",
    "- a second graph convolutional layer, which maps the dimensions down to the number of classes.\n",
    "\n",
    "*Hint*: Check out the pytorch_geometric documentation to check the inputs and outputs of the graph convolutional layers.\n",
    "\n",
    "Visualize your untrained node embeddings using the `visualize` function given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: fill in the __init__() and forward() functions. Add arguments if needed.\n",
    "class NodeClassificationModel(torch.nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(NodeClassificationModel, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "fmXWs1dKIzD8",
    "outputId": "3646967f-a64c-4705-b9a6-2824ce2b7dc7",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class NodeClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features, num_hidden_layers):\n",
    "        super(NodeClassificationModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_features, hidden_dimensions)\n",
    "        self.conv2 = GCNConv(hidden_dimensions, out_features)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden_layers = []\n",
    "        if num_hidden_layers > 0:  # for the 3rd task\n",
    "            for i in range(num_hidden_layers):\n",
    "                self.hidden_layers += [GCNConv(hidden_dimensions, hidden_dimensions)]\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        if len(self.hidden_layers) > 0: # for the 3rd task\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                x = hidden_layer(x, edge_index)\n",
    "                x = self.relu(x)\n",
    "                x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "ncm = NodeClassificationModel(in_features=dataset.num_features, out_features=dataset.num_classes,\n",
    "                                hidden_dimensions=16, num_hidden_layers=4)\n",
    "print(ncm)\n",
    "ncm.eval()\n",
    "\n",
    "model_output = ncm(data.x, data.edge_index)\n",
    "visualize(model_output, color=data.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpdscco5g6kG"
   },
   "source": [
    "#### 2. Train the model\n",
    "Use cross-entropy loss and the adam optimizer with a learning rate of 0.01 and L2 regularization with a penalty of $5\\cdot 10^{-4}$. Train the model for a maximum of 200 epochs while using the validation data to implement early stopping. Evaluate your model accuracy on the test data.\n",
    "\n",
    "Visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Train and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3TAi69zI1bO",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, data, model, optimizer, loss_function, stop_criterion):\n",
    "    best_val_loss = 100000\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        train_loss = loss_function(out[data.train_mask], data.y[data.train_mask])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = evaluate(out, data.y, loss_function, data.val_mask)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'GCN.pt')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if epoch %5 == 0:\n",
    "            print(f\"Epoch {epoch} \\t Train Loss: {train_loss:.4f} \\t Val Loss: {val_loss:.4f} \\t Val Acc: {val_acc:.4f}\")\n",
    "        if counter > stop_criterion:\n",
    "            break\n",
    "    model.load_state_dict(torch.load('GCN.pt'))\n",
    "\n",
    "def evaluate(model_output, ground_truth, loss_function, split):\n",
    "\n",
    "    split_loss = loss_function(model_output[split], ground_truth[split])\n",
    "    prediction = model_output.argmax(dim=1)\n",
    "    correct = (prediction[split]==ground_truth[split]).sum()\n",
    "    accuracy = correct / split.sum()\n",
    "    return split_loss, accuracy\n",
    "                  \n",
    "ncm = NodeClassificationModel(in_features=dataset.num_features, out_features=dataset.num_classes,\n",
    "                                hidden_dimensions=16, num_hidden_layers=0)\n",
    "optimizer = torch.optim.Adam(ncm.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "train(200, data, ncm, optimizer, loss_function, 20)\n",
    "\n",
    "ncm.eval()\n",
    "model_output = ncm(data.x, data.edge_index)\n",
    "test_loss, test_acc = evaluate(model_output, data.y, loss_function, data.test_mask)\n",
    "print(f\"Test Accuracy: {test_acc.item()*100:.2f}%\")\n",
    "visualize(model_output, color=data.y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S-q6Do4INLET"
   },
   "source": [
    "#### 3. More complex models (optional)\n",
    "\n",
    "Modify your Graph Neural Network. What happens if you increase the hidden dimension or use more layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXj9gdE_KWCa"
   },
   "source": [
    "### 2. Graph Classification\n",
    "\n",
    "Now that we build a model to classify nodes in a graph successfully, we turn to graph classification. Here we now want to create representations - and then predict the labels - for entire graphs.\n",
    "\n",
    "As the official [tutorial](https://colab.research.google.com/drive/1I8a0DfQ3fI7Njc62__mVXUlcAleUclnb) we use the `MUTAG` dataset. Each graph represents a chemical which have to be classified into two classes depending how they affect a bacterium. The data loading is again done for you. pytorch-geometric also provides its own dataloader, which takes care of any difficulties when creating batches for graphs. Check out the official tutorial, if you want to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J94z8tgKKxOq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "dataset = TUDataset(root='./data/TUDataset', name='MUTAG')\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "print(f\"Number of node features: {dataset.num_node_features}, Number of edge features: {dataset.num_edge_features}\")\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:100]\n",
    "val_dataset = dataset[100:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n",
    "print()\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "print()\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-8lhqvTK2Xn"
   },
   "source": [
    "As you can see, in contrast to the `Cora` dataset, here we have multiple -- but smaller -- graphs and each graph has just one label. Additionally we have edge features (`edge_attr`).\n",
    "\n",
    "Note that the batched data contains a new attribute `batch`, which maps each node to its respective graph in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Build a Graph Neural Network\n",
    "\n",
    "To create representations for entire graphs, we first create embeddings for each node, which are then aggregated into an embedding for the entire graph in a so-called **readout layer**.\n",
    "\n",
    "Build a graph neural network consisting of:\n",
    "1. A `torch_geometric.nn.GINCov` layer. Use `torch.nn.Linear` for the nn, which maps the input from the number of node features to 64 hidden dimensions.\n",
    "2. Two further `torch_geometric.nn.GINCov` with `torch.nn.Linear`, keeping in and out features at 64 dimensions.\n",
    "3. Apply ReLU between the convolutional layers.\n",
    "4. Aggregate the output of the last convolutional layer into a graph representation using `torch_gemoentric.nn.global_mean_pool`\n",
    "5. Apply a dropout layer with dropout probability of 0.5\n",
    "6. Apply a fully connected layer, where the out features are equal to the number of classes.\n",
    "\n",
    "*Hint*: Make sure that the pooling layer knows which nodes belong to which graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "#ToDo: fill in the __init__ and forward functions. Add arguments if needed.\n",
    "class GraphClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(GraphClassificationModel, self).__init__()\n",
    "        \n",
    "\n",
    "    def forward(self, ):\n",
    "        \n",
    "        \n",
    "        return x\n",
    "\n",
    "gcm = GraphClassificationModel()\n",
    "print(gcm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6C4ZDbVSLREA",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import GINConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GraphClassificationModel(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features):\n",
    "        super(GraphClassificationModel, self).__init__()\n",
    "        self.conv1 = GINConv(nn=nn.Linear(in_features, hidden_dimensions))\n",
    "        self.conv2 = GINConv(nn=nn.Linear(hidden_dimensions, hidden_dimensions))\n",
    "        self.conv3 = GINConv(nn=nn.Linear(hidden_dimensions, hidden_dimensions))\n",
    "        self.fc = nn.Linear(hidden_dimensions, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "gcm = GraphClassificationModel(in_features=dataset.num_node_features, out_features=dataset.num_classes,\n",
    "                              hidden_dimensions=64)\n",
    "print(gcm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRZlvdsDLUKw"
   },
   "source": [
    "#### 2. Train the model\n",
    "Use cross-entropy loss and the adam optimizer with a learning rate of 0.01 and L2 regularization with a penalty of $5\\cdot 10^{-4}$. Train the model for a maximum of 200 epochs while using the validation data to implement early stopping. Evaluate your model accuracy on the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implemnt Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuMslFFOLXCs",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, optimizer, loss_function, stop_criterion):\n",
    "    best_val_loss = 100000\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            out = model(data.x, data.edge_index, data.batch)\n",
    "            train_loss = loss_function(out, data.y)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, loss_function)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'GCM.pt')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if epoch %5 == 0:\n",
    "            print(f\"Epoch {epoch} \\t Train Loss: {train_loss:.4f} \\t Val Loss: {val_loss:.4f} \\t Val Acc: {val_acc:.4f}\")\n",
    "        if counter > stop_criterion:\n",
    "            break\n",
    "    model.load_state_dict(torch.load('GCM.pt'))\n",
    "\n",
    "def evaluate(data_loader, model, loss_function):\n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tp = 0\n",
    "    num_entries = 0\n",
    "    for data in data_loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        loss = loss_function(out, data.y)\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        prediction = out.argmax(dim=1)\n",
    "        tp += (prediction == data.y).sum()\n",
    "        num_entries += len(data)\n",
    "    return cum_loss/len(data_loader), tp/num_entries\n",
    "\n",
    "\n",
    "gcm = GraphClassificationModel(in_features=dataset.num_node_features, out_features=dataset.num_classes,\n",
    "                              hidden_dimensions=64)\n",
    "\n",
    "optimizer = torch.optim.Adam(gcm.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "train(200, train_loader, val_loader, gcm, optimizer, loss_function, 20)\n",
    "\n",
    "test_loss, test_acc = evaluate(test_loader, gcm, loss_function)\n",
    "print(f\"Accuracy on Test Set: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMFXF4DDLfNS"
   },
   "source": [
    "#### 3. Graph Neural Networks with Edge Attributes\n",
    "\n",
    "As mentioned before, our dataset also contains edge attributes. Now we want to make use of them.\n",
    "\n",
    "Replace the `torch_geometric.nn.GINCov` by `torch_geometric.nn.GINECov`, which can deal with edge attributes. Train your model again. Can you achieve better performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "#ToDo: fill the __init__ and forward functions, add arguments if needed.\n",
    "class GraphClassificationModelwE(torch.nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(GraphClassificationModelwE, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, ):\n",
    "\n",
    "        return x\n",
    "\n",
    "gcmwe = GraphClassificationModelwE()\n",
    "print(gcmwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Nymp730LhoO",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class GraphClassificationModelwE(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_dimensions, out_features, edge_features):\n",
    "        super(GraphClassificationModelwE, self).__init__()\n",
    "        self.conv1 = GINEConv(nn=nn.Linear(in_features, hidden_dimensions), edge_dim=edge_features)\n",
    "        self.conv2 = GINEConv(nn=nn.Linear(hidden_dimensions, hidden_dimensions), edge_dim=edge_features)\n",
    "        self.conv3 = GINEConv(nn=nn.Linear(hidden_dimensions, hidden_dimensions), edge_dim=edge_features)\n",
    "        self.fc = nn.Linear(hidden_dimensions, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "gcmwe = GraphClassificationModelwE(in_features=dataset.num_node_features, out_features=dataset.num_classes,\n",
    "                              hidden_dimensions=64, edge_features=dataset.num_edge_features)\n",
    "print(gcmwe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Implement training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okxGkaDiLmX2",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, train_dataloader, val_dataloader, model, optimizer, loss_function, stop_criterion):\n",
    "    best_val_loss = 100000\n",
    "    counter = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for data in train_dataloader:\n",
    "            out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "            train_loss = loss_function(out, data.y)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, loss_function)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'GCM.pt')\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        if epoch %5 == 0:\n",
    "            print(f\"Epoch {epoch} \\t Train Loss: {train_loss:.4f} \\t Val Loss: {val_loss:.4f} \\t Val Acc: {val_acc:.4f}\")\n",
    "        if counter > stop_criterion:\n",
    "            break\n",
    "    model.load_state_dict(torch.load('GCM.pt'))\n",
    "\n",
    "def evaluate(data_loader, model, loss_function):\n",
    "    model.eval()\n",
    "    cum_loss = 0\n",
    "    tp = 0\n",
    "    num_entries = 0\n",
    "    for data in data_loader:\n",
    "        out = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
    "        loss = loss_function(out, data.y)\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        prediction = out.argmax(dim=1)\n",
    "        tp += (prediction == data.y).sum()\n",
    "        num_entries += len(data)\n",
    "    return cum_loss/len(data_loader), tp/num_entries\n",
    "\n",
    "\n",
    "gcmwe = GraphClassificationModelwE(in_features=dataset.num_node_features, out_features=dataset.num_classes,\n",
    "                              hidden_dimensions=64, edge_features=dataset.num_edge_features)\n",
    "\n",
    "optimizer = torch.optim.Adam(gcmwe.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "train(200, train_loader, val_loader, gcmwe, optimizer, loss_function, 20)\n",
    "\n",
    "test_loss, test_acc = evaluate(test_loader, gcmwe, loss_function)\n",
    "print(f\"Accuracy on Test Set: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "11_Node_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python DL-LAI",
   "language": "python",
   "name": "python-dl_lai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "346px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
