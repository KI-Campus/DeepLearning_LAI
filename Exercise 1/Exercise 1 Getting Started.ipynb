{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntaR_qGpvUgg"
   },
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "aK_rtWRkvUgh"
   },
   "source": [
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
    "\n",
    "- A 0d tensor is a scalar,\n",
    "- A 1d tensor is a vector (e.g. a sound sample),\n",
    "- A 2d tensor is a matrix (e.g. a grayscale image),\n",
    "- A 3d tensor can be seen as a vector of identically sized matrix (e.g. a\n",
    "multi-channel image),\n",
    "- A 4d tensor can be seen as a matrix of identically sized matrices, or a\n",
    "sequence of 3d tensors (e.g. a sequence of multi-channel images),\n",
    "- etc\n",
    "\n",
    "There is a confusion between “dimension” for a vector in linear algebra, which is its number of coefficients, and “dimension” for a tensor, which is the number of indices to specify one of its coefficients. For instance an element of $\\mathbb{R}^3$ is a three-dimension vector, but a one-dimension tensor.\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other specialized hardware to accelerate computing. Manipulating data through this constrained structure allows to use CPUs\n",
    "and GPUs at peak performance.\n",
    "\n",
    "For example, an RGB image of H rows and W columns of pixels can be encoded as a 3d tensor of size 3 × H × W, or depending on the convention H × W × 3.\n",
    "\n",
    "A series of N images can thus be encoded as a\n",
    "single tensor of size N × 3 × H × W.\n",
    "\n",
    "### PyTorch’s main features are:\n",
    "- Efficient tensor operations on CPU/GPU,\n",
    "- automatic on-the-fly differentiation (autograd),\n",
    "- optimizers,\n",
    "- data I/O.\n",
    "\n",
    "**“Efficient tensor operations”** encompass both standard linear algebra and, as we will see\n",
    "later, deep-learning specific operations (convolution, pooling, etc.). A key specificity of PyTorch is the central role of **autograd** to compute derivatives of anything! (We will come back to this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZOQZtWZvUgi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(1234567) #fix the random seed (for the reproducability of your results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2KxfgrervUgj"
   },
   "source": [
    "## Tensor Initialization\n",
    "Tensors can be initialized in various ways. Take a look at the following examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2w537nqrvUgj"
   },
   "source": [
    "### Directly from data\n",
    "\n",
    "Tensors can be created directly from data. The data type is automatically inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VLuyzXMTvUgj",
    "outputId": "b3f9ded2-4394-40c2-d1e1-3705c072df56"
   },
   "outputs": [],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Uk74gmmKvUgk"
   },
   "source": [
    "### From a NumPy array\n",
    "\n",
    "Tensors can be created from NumPy arrays (and vice versa - see [Bridge with NumPy](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA8SY0BUvUgk",
    "outputId": "3f700ae3-00bf-4f7b-ca52-f04344dcfa30"
   },
   "outputs": [],
   "source": [
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "print(np_array)\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pxFImQDBvUgl"
   },
   "source": [
    "### With pytorch\n",
    "\n",
    "There are lots of options creating tensors using `torch.*`\n",
    "\n",
    "`shape` is a tuple of tensor dimensions. In the functions below, it determines the dimensionality of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCyIz5j3vUgl",
    "outputId": "26c0f78c-be34-4005-ac97-5be8609a7a71"
   },
   "outputs": [],
   "source": [
    "shape = (2,3,)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "print(rand_tensor)\n",
    "print(ones_tensor)\n",
    "print(zeros_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tvJcn6fYvUgl"
   },
   "source": [
    "### From another tensor:\n",
    "\n",
    "The new tensor retains the properties (shape, datatype) of the argument tensor, unless explicitly overridden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZNNbgiT_vUgm",
    "outputId": "a0659b4e-6dcd-4c59-dc03-a891d9b44dd1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
    "print(x_ones)\n",
    "print(x_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IL0apriOvUgp"
   },
   "source": [
    "### Back to scalars, lists, numpy arrays:\n",
    "A 0d tensor can be converted back to a Python scalar with `.item()`. Any-dimensional tensors can be converted to lists by `.tolist()` or to NumPy arrays by `.numpy()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Csdt_ZsqvUgq",
    "outputId": "49c236e8-d85f-4991-84f7-77fa643e0981"
   },
   "outputs": [],
   "source": [
    "print(x_rand[0,0])\n",
    "print(x_rand[0,0].item())\n",
    "print(x_rand.tolist())\n",
    "print(x_rand.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qx_j01bDvUgm"
   },
   "source": [
    "## Tensor Attributes\n",
    "\n",
    "Tensor attributes describe their shape, datatype, and the device on which they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvT253e1vUgm",
    "outputId": "36bc3ad6-9da6-40b3-de0c-50a0512c9b39"
   },
   "outputs": [],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dfx2EgLQvUgm"
   },
   "source": [
    "A tensor can be of many different data types. Take a look at the official documentation: [torch.tensor](https://pytorch.org/docs/stable/tensors.html), where you will find a large table of data types. The most important ones for now are:\n",
    "- torch.float / torch.float32\n",
    "- torch.long / torch.int64\n",
    "- torch.bool\n",
    "\n",
    "and their corresponding cuda variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFEg1EcLvUgn",
    "outputId": "aebbab49-7819-45a2-faaa-0f37052cc279"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([1,0,1,0,1])\n",
    "print(t)\n",
    "print(t.float())\n",
    "print(t.bool())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CCGZtlxXvUgo"
   },
   "source": [
    "You can move tensors to the GPU, where many operations can be run (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU via `Runtime -> Change runtime type -> Hardware Accelerator -> GPU`. On Colab and linux-based systems, you can get GPU info by running `nvidia-smi`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhYnEb30vUgo",
    "outputId": "21e71c65-46a1-40ca-a438-8301bb9b8de4"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8VDhgICvUgo",
    "outputId": "89bf2149-c248-4bc8-b330-94007f5e56d6"
   },
   "outputs": [],
   "source": [
    "# We move our tensor to the GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to('cuda')\n",
    "    print(tensor) # notice that the tensor's device is now 'cuda:X', with X indicating the gpu id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dhbsHpAqvUgn"
   },
   "source": [
    "# Exercise: Working with Tensors\n",
    "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are comprehensively described in the [pytorch docs](https://pytorch.org/docs/stable/torch.html). Many of them are similar to the numpy API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV9dyZVLvUgo"
   },
   "source": [
    "## Simple Operations\n",
    "\n",
    "Create a Tensor $ x = \\begin{pmatrix} 1 & 2 &3 \\\\ 4 & 5 & 6 \\end{pmatrix}$ and calculate the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Sum over all entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Sum only over dimension 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Substract 1 from every element of $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** $x+x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Square $x$ elementwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5YvLiycLvUgp",
    "outputId": "9b4bf7d4-55df-45ae-e047-5f177bfeefe9",
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x.pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3vBYdhWvUgp"
   },
   "source": [
    "In-place operations are suffixed with an underscore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSO7QtMMvUgp",
    "outputId": "43e6cb69-0a12-45a1-be06-d0d1eb55ba13"
   },
   "outputs": [],
   "source": [
    "x_test = torch.ones(2)\n",
    "x_test.add_(4)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D69XXzK7vUgq"
   },
   "source": [
    "## Indexing and Slicing\n",
    "You can apply standard NumPy indexing and Slicing to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(np.arange(16)).reshape(4,4)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Print element $x_{2,1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "print(x[2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Set all elements of column 1 to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "x[:, 1] = 0 # colon means select all from this dimension. here: all rows (dim=0), first column (dim=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenation and Stacking of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0,6, (3,3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wz4zBV0SvUgq",
    "outputId": "0c0ecd87-0814-4e31-873f-016e531d182c"
   },
   "source": [
    "**a)** Concatenate / Stack the tensor x multiple times to create a new tensor of shape (3, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x_1 = torch.cat([x, x, x], dim=1)\n",
    "print(x_1.shape)\n",
    "print(x_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Concatenate / Stack the tensor x multiple times to create a new tensor of shape (9, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x_2 = torch.cat([x, x, x], dim=0)\n",
    "print(x_2.shape)\n",
    "print(x_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c)** Concatenate / Stack the tensor x multiple times to create a new tensor of shape (2, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x_3 = torch.stack([x, x], dim=0)\n",
    "print(x_3.shape)\n",
    "print(x_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d)** Concatenate / Stack the tensor x multiple times to create a new tensor of shape (3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x_4 = torch.stack([x, x], dim=-1)\n",
    "print(x_4.shape)\n",
    "print(x_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e)** Concatenate / Stack the tensor x multiple times to create a new tensor of shape (3, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "x_5 = torch.stack([x, x], dim=1)\n",
    "print(x_5.shape)\n",
    "print(x_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79ZAvjyGvUgr"
   },
   "source": [
    "## Multiplication of Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, 3, (3,2))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a)** Compute the elementwise product $x\\cdot x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4Zcxy85vUgr",
    "outputId": "f256a4d2-fcf5-49f7-e3f1-be2175b70d8d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.mul(x)) # Alternative syntax: x*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** Compute the matrix multiplication $x x^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJfX9UnwvUgs",
    "outputId": "f56a4504-328c-4fcd-9c89-453b4abdd480"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.matmul(x.T)) # Alternative syntax: tensor @ tensor.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Notes on Datatypes and Tensor Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geITim-BvUgt"
   },
   "source": [
    "Some operations only work with specific datatypes. For example, computing the mean of a tensor cannot be executed on a int64 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167
    },
    "id": "mFhBi_ovvUgt",
    "outputId": "b0969b94-485b-42d9-d401-7ab7b82dbfea"
   },
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3,4,5,6]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFCycO9NvUgt"
   },
   "source": [
    "We can fix this by converting to floats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqXbncv2vUgt",
    "outputId": "c84e4b14-0f6b-4563-923e-fe65857e8053"
   },
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3,4,5,6]).float().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1fJsoOIvUgu"
   },
   "source": [
    "Sometimes, a tensor does not have the shape we desire. `squeeze` and `unsqueeze` can help removing or adding dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcQ2FiLdvUgu",
    "outputId": "0c21c4de-7449-480a-8230-67faa4ff6939"
   },
   "outputs": [],
   "source": [
    "t = torch.tensor([[1,2,3]])\n",
    "print(t.shape)\n",
    "print(t.unsqueeze(0).shape) # new dimension at location 0\n",
    "print(t.squeeze(0).shape) # remove dimension at location 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RI_WH7T9vUgu"
   },
   "source": [
    "`view` or `reshape` can change the shape of a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjvsfOx3vUgu",
    "outputId": "482cdc96-4b9a-46c2-ee63-5ba61d081d4c"
   },
   "outputs": [],
   "source": [
    "print(t.view((3,1)).shape) # Note that view returns a \"view on the tensor\" (that means shared underlying memory),\n",
    "print(t.reshape((3,1)).shape)          # while reshape can return a copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-Ea9YsDvUgu"
   },
   "source": [
    "Applying the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfk_EFSrvUgv",
    "outputId": "f7f6d9e6-c9b7-48fd-dd3c-821be2cd5ceb"
   },
   "outputs": [],
   "source": [
    "torch.tensor([1,2,3]).sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0h01oPbMvUgv"
   },
   "source": [
    "The is a very large number of tensor operations available. A full list is documented [here](https://pytorch.org/docs/stable/tensors.html). Whenever you want to perform some operation, we recommend you taking a look at this documentation and search through it to find what fits your needs. Overtime, you will feel more and more comfortable and remember many operations without having to look at the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5DI9q2UXw3v"
   },
   "source": [
    "## Broadcasting\n",
    "**Broadcasting** auto-magically expands dimensions by replicating coefficients, when it is necessary to perform operations that are “intuitively reasonable”. For example, it allows you to add a tensor $b$ of shape (3,) to a tensor $a$ of shape (2, 3) by replicating $b$ across the first dimension of $a$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYuBFCI_Xw3v",
    "outputId": "d2eaf6ff-7bdd-48ec-cade-f7010ea6a1f1"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "b = torch.tensor([1, 2, 3])\n",
    "a.shape, b.shape, a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMDfEIiHXw3v"
   },
   "source": [
    "While broadcasting can simplify many operations, it can also lead to unintended behavior and thus unnoticed bugs.\n",
    "Try around with different shapes of a and b and different operations. We recommend to also at least skim read through the [documentation page of pytorch's Broadcasting semantics](https://pytorch.org/docs/stable/notes/broadcasting.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Tensors.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "dl_lai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
