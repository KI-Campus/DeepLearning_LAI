{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "liable-adams",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "## Exercise 12 - Interpretability\n",
    "\n",
    "In this notebook we will look at different interpretability techniques and principles. The code examples are based on Captum tutorials you can find at https://captum.ai/tutorials/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea563e",
   "metadata": {},
   "source": [
    "### 1. Tabular Data\n",
    "\n",
    "The first task uses feature vectors. We use datapoints about the survivors and victims of the Titanic crash to find out which properties predict a high likelihood of survival.\n",
    "\n",
    "First we prepare the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(12345)\n",
    "torch.manual_seed(1)\n",
    "df_train = pd.read_csv(\"https://huggingface.co/datasets/Javitron4257/Titanic-Dataset/resolve/main/titanic_train.csv\")\n",
    "\n",
    "\n",
    "# Convert categorical data to one-hot encoding\n",
    "df_train = pd.concat([df_train,\n",
    "                              pd.get_dummies(df_train['Sex']),\n",
    "                              pd.get_dummies(df_train['Embarked'],prefix=\"Embark\"),\n",
    "                              pd.get_dummies(df_train['Pclass'],prefix=\"Class\")], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Replace unknown age values with mean of the dataset\n",
    "df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].mean())\n",
    "\n",
    "rel_columns = ['Age', 'SibSp', 'Parch',\n",
    "               'male', 'female',\n",
    "               'Embark_S', 'Embark_Q', 'Embark_C',\n",
    "               'Class_1', 'Class_2', 'Class_3']\n",
    "print(df_train[rel_columns])\n",
    "\n",
    "train_labels = df_train['Survived'].to_numpy()\n",
    "train_features = df_train[rel_columns].to_numpy()\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(train_features, train_labels,\n",
    "                                                                            train_size=0.7, stratify=train_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-expansion",
   "metadata": {},
   "source": [
    "After this, each vector in the dataset contains:\n",
    "\n",
    "* age of the passenger\n",
    "* number of siblings and spouses on board\n",
    "* number of parents and children on board\n",
    "* one-hot-encoded sex of the passenger (male or female)\n",
    "* one-hot-encoded place of embarkment (Cherbourgh, Queenstown or Southhampton)\n",
    "* one-hot-encoded class in which the passenger traveled (first to third class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071be23c",
   "metadata": {},
   "source": [
    "#### 1. Build and Train a NN\n",
    "Build a simple Neural network consisting of three fully connected layers going from 11 to 16, 16 to 16 and 16 to 1 dimensions. Use sigmoid as an activation after each fully connected layer. Train the model for 200 epochs, using binary cross-entropy loss and the adam optimizer with a learning rate of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194600e1",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#ToDo: fill the __init__ and forward functions. Add arguments if needed.\n",
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-queens",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TitanicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(11, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.flatten()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf87459",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Train the model and test it on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cdca88",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "tm = TitanicModel()\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(tm.parameters(), lr=0.1)\n",
    "\n",
    "train_f = torch.tensor(train_features, dtype=torch.float)\n",
    "train_l = torch.tensor(train_labels, dtype=torch.float)\n",
    "\n",
    "test_f = torch.tensor(test_features, dtype=torch.float)\n",
    "test_l = torch.tensor(test_labels, dtype=torch.float)\n",
    "\n",
    "def train(num_epochs, model, train_features, train_labels, opimizer, loss_function):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        output = model(train_features)\n",
    "        loss = loss_function(output, train_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch} \\t -- \\t Loss {loss:.4f}\")\n",
    "\n",
    "def evaluate(model, features, labels, loss_function):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(features)\n",
    "        loss = loss_function(output, labels)\n",
    "        \n",
    "        predictions = (output > 0.5)\n",
    "        accuracy = (predictions==labels).sum()/labels.shape[0]\n",
    "    return loss.item(), accuracy\n",
    "\n",
    "train(200, tm, train_f, train_l, optimizer, loss_function)\n",
    "test_loss, test_accuracy = evaluate(tm, test_f, test_l, loss_function)\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-valuation",
   "metadata": {},
   "source": [
    "#### 2. Integrated Gradients\n",
    "\n",
    "Integrated gradients is a feature attribution method based on the gradients with respect to the input. It is smoother than just the gradients as it integrates the gradients starting from a base representation to the actual input.\n",
    "\n",
    "Use `captum.attr.IntegratedGradients` to extract feature attributions on the test set. Visualize the average attributions and the attributions for the first instance in the test set.\n",
    "\n",
    "*Hint*: As the method is based on gradients with respect to the input, your input now requires gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac423c",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ToDo: Apply IntegratedGradients to the model and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-season",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ig = IntegratedGradients(tm)\n",
    "\n",
    "# integrated gradients requires the input gradient, so enable that in the input tensor\n",
    "test_f.requires_grad_()\n",
    "\n",
    "# retrieve attributions\n",
    "attr = ig.attribute(test_f).detach().numpy()\n",
    "\n",
    "# Helper method to print importances and visualize distribution\n",
    "def visualize_importances(net, feature_names, importances, title=\"Average Feature Importances\", axis_title=\"Features\"):\n",
    "    x_pos = (np.arange(len(feature_names)))\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.bar(x_pos, importances, align='center')\n",
    "    plt.xticks(x_pos, feature_names, wrap=True)\n",
    "    plt.xlabel(axis_title)\n",
    "    plt.title(title)\n",
    "\n",
    "visualize_importances(tm, rel_columns, np.mean(attr, axis=0))\n",
    "passnum = 0\n",
    "for name, value in zip(rel_columns, test_f[passnum]):\n",
    "    print(f\"Attribute: {name} - Value: {value}\")\n",
    "print(f'Predicted survival probability: {tm(test_f[passnum].unsqueeze(dim=0)).item():.4f} - Actual Survival: {test_labels[passnum] == 1}')\n",
    "visualize_importances(tm, rel_columns, attr[passnum], title='Passenger {}'.format(passnum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-genealogy",
   "metadata": {},
   "source": [
    "How do the individual features impact the model on average? Which are the most relevant ones? What do you get from one specific passenger?\n",
    "\n",
    "You will note that many features have an attribution score of zero. Integrated gradients as a change between the model output for the baseline (in this case the zero vector) and the data point. If the feature value is identical to the baseline, it is attributed no score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-vision",
   "metadata": {},
   "source": [
    "#### 3. KernelSHAP\n",
    "\n",
    "KernelSHAP is a different feature attribution technique. It is based on [Shapley values](https://en.wikipedia.org/wiki/Shapley_value), which are grounded in game theory. The idea behind Shapley values is to determine in a cooperative game, how much each player contributed to the outcome (e.g. was player 1 very helpful while it wouldn't have made a difference if player 2 hadn't participated). More on the adaptation to explainability can be found in [this paper](https://papers.nips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf).\n",
    "\n",
    "Use `captum.attr.KernelShap` to extract feature attributions on the test set. Visualize the average attributions and the attributions for the first instance in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import KernelShap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ac48d",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# Apply KernelSHAP to the model and visualize the resutls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-place",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from captum.attr import KernelShap\n",
    "\n",
    "shap = KernelShap(tm)\n",
    "\n",
    "shapattr = shap.attribute(test_f)\n",
    "shapattr = shapattr.detach().numpy()\n",
    "\n",
    "visualize_importances(tm, rel_columns, np.mean(shapattr, axis=0))\n",
    "visualize_importances(tm, rel_columns, shapattr[passnum], title='Passenger {}'.format(passnum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-atlantic",
   "metadata": {},
   "source": [
    "How do the attribution scores compare to the ones derived by Integrated Gradients? Are they similar or different? Can you think about why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-drain",
   "metadata": {},
   "source": [
    "### 2. Image Data\n",
    "\n",
    "Now we want to also look at image data. We want to explain a large, pre-trained model - ResNet, which was trained on the ImageNet data.\n",
    "\n",
    "This part of the exercise is based on a tutorial from [captum](https://captum.ai/tutorials/TorchVision_Interpret). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fdff55",
   "metadata": {},
   "source": [
    "First we will load the pre-trained model and the descriptions of the labels contained in ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dangerous-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import models\n",
    "import json\n",
    "import requests\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "imagenet_labels_raw = requests.get('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json').text\n",
    "imagenet_labels = json.loads(imagenet_labels_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-butterfly",
   "metadata": {},
   "source": [
    "Then download an image of a Ford Model T from Wikipedia and transform it into a tensor that resnet can handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "raw_image = Image.open(requests.get('https://upload.wikimedia.org/wikipedia/commons/1/15/Late_model_Ford_Model_T.jpg', stream=True).raw)\n",
    "\n",
    "display(raw_image)\n",
    "\n",
    "# Resize image to fit the resnet's expected input resolution\n",
    "# Transform in two steps because we need the unnormalized image for visualization below\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Normalize the color channels to fit the color channel distributions of the imagenet training set.\n",
    "transform_normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "transformed_img = transform(raw_image)\n",
    "input_img = transform_normalize(transformed_img)\n",
    "input_batch = input_img.unsqueeze(0)\n",
    "\n",
    "print(input_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-secondary",
   "metadata": {},
   "source": [
    "Next we'll extract the model prediction. The model predicts that the image shows a Model T with 97.8% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-providence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "model_out = model(input_batch)\n",
    "output = F.softmax(model_out, dim = 1)\n",
    "prediction_score, pred_label_idx = torch.topk(output, 1)\n",
    "\n",
    "pred_label_idx.squeeze_()\n",
    "prediction_score.squeeze_()\n",
    "\n",
    "predicted_label = imagenet_labels[str(pred_label_idx.item())][1]\n",
    "\n",
    "print(\"Prediction:\", predicted_label)\n",
    "print(\"Confidence: \", prediction_score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3706e18",
   "metadata": {},
   "source": [
    "#### 1. Integrated Gradients\n",
    "\n",
    "Compute the feature attributions using Integrated Gradients. Visualize the output using `captum.attr.visualization.visualize_image_attr()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812cf9f9",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "#ToDo: calculate attributions and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb827e11",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "# integrated gradients requires the input gradient, so enable that in the input tensor\n",
    "input_batch.requires_grad_()\n",
    "\n",
    "# retrieve attributions\n",
    "attr = ig.attribute(input_batch, target=pred_label_idx).detach().numpy()\n",
    "\n",
    "_ = viz.visualize_image_attr(np.transpose(attr.squeeze(), (1,2,0)),\n",
    "                             np.transpose(input_img.cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='heat_map',\n",
    "                             show_colorbar=True,\n",
    "                             cmap='Reds',\n",
    "                             sign='positive',\n",
    "                             outlier_perc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba01a6e",
   "metadata": {},
   "source": [
    "#### 2. Improving Integrated Gradients\n",
    "\n",
    "As you can see, the attribution result isn't that comprehensible. We want to improve the result by using `captum.attr.NoiseTunnel`, to calculate the attribution not just for one image, but 10 noisy variants of the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631081a",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "#ToDo: Calculate the attributions and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47b334",
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ig = NoiseTunnel(IntegratedGradients(model))\n",
    "\n",
    "# integrated gradients requires the input gradient, so enable that in the input tensor\n",
    "input_batch.requires_grad_()\n",
    "\n",
    "# retrieve attributions\n",
    "attr = ig.attribute(input_batch, target=pred_label_idx, nt_type='smoothgrad_sq', nt_samples=10).detach().numpy()\n",
    "\n",
    "_ = viz.visualize_image_attr(np.transpose(attr.squeeze(), (1,2,0)),\n",
    "                             np.transpose(input_img.cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='heat_map',\n",
    "                             show_colorbar=True,\n",
    "                             cmap='Reds',\n",
    "                             sign='positive',\n",
    "                             outlier_perc=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b976da",
   "metadata": {},
   "source": [
    "Can you now see which parts of the image have the most influence on the models decision? Try higher numbers of noisy images (be careful of the runtime), to get even clearer images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262399bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python DL-LAI",
   "language": "python",
   "name": "python-dl_lai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "342.467px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
