{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saItxTU2Vs9p"
   },
   "source": [
    "# Deep Learning\n",
    "## Exercise 6 - Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3iVdKn-jFNa7"
   },
   "source": [
    "### 1. Effect of Momentum\n",
    "Momentum can accelerate the training when the loss surface is too flat. Another effect of momentum is avoiding the optimization algorithm from being trapped in a local minimum and help to find the global minimum. Assume our loss function to be\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\theta)=\n",
    "    \\begin{cases}\n",
    "        \\theta \\cdot cos(\\pi \\theta) & \\text{if } âˆ’1.0\\leq \\theta\\leq 2.0 \\\\\n",
    "        1e10 & \\text{else}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "where $\\theta$ is the parameter to solve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWZ27K5RE6tm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "import numpy as np\n",
    "\n",
    "def loss_function(theta):\n",
    "    loss = torch.full(theta.shape, 1e10)\n",
    "    loss[(theta>=-1)&(theta<=2)] = theta * torch.cos(theta*pi)\n",
    "    return loss\n",
    "\n",
    "x = torch.linspace(-1, 2, 50)\n",
    "\n",
    "plt.plot(x, loss_function(x))\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Theta')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. SGD without Momentum\n",
    "\n",
    "Using pytorch's `torch.optim.SGD`, find a minimum with initial values of $\\theta = -0.95$ and learning rate $\\alpha = 0.01$. Does your algorithm get stuck at a local minimum? Plot the loss values over the number of steps and on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run SGD without momentum. How do your loss values evolve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "theta = torch.tensor([-0.95], requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([theta], lr = 0.01)\n",
    "\n",
    "def run_SGD(theta, optimizer, loss_function):\n",
    "    loss = loss_function(theta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "theta_steps, loss_values = [],[]\n",
    "\n",
    "for i in range(100):\n",
    "    theta_steps += [theta.item()]\n",
    "    loss_values += [run_SGD(theta, optimizer, loss_function).item()]\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "    \n",
    "ax1.plot(list(range(100)), loss_values, 'o', color='tab:orange')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Step')\n",
    "ax2.plot(x, loss_function(x))\n",
    "ax2.plot(theta_steps, loss_values, 'o')\n",
    "ax2.set_xlabel('Theta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SGD with Momentum\n",
    "\n",
    "Use Momentum (by setting the momentum argument). Which value for the momentum lets the algorithm converge at the global minimum? Plot the loss values over the number of steps and on the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run SGD with momentum. How do your loss values evolve now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "theta = torch.tensor([-0.95], requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([theta], lr = 0.01, momentum=0.95)\n",
    "\n",
    "def run_SGD(theta, optimizer, loss_function):\n",
    "    loss = loss_function(theta)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss\n",
    "\n",
    "theta_steps, loss_values = [],[]\n",
    "\n",
    "for i in range(100):\n",
    "    theta_steps += [theta.item()]\n",
    "    loss_values += [run_SGD(theta, optimizer, loss_function).item()]\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(1,2, sharey=True, figsize=(10,5))\n",
    "    \n",
    "ax1.plot(list(range(100)), loss_values, 'o', color='tab:orange')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Step')\n",
    "ax2.plot(x, loss_function(x))\n",
    "ax2.plot(theta_steps, loss_values, 'o')\n",
    "ax2.set_xlabel('Theta')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMxIzEdOE6tq"
   },
   "source": [
    "### 2. Effect of Optimization Techniques on Training Time and Performance\n",
    "In this task we revisit the CNN from Exercise 4.2, that you built to classify images into the 10 classes of the CIFAR-10 dataset. You can re-use your solutions for that taks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqFMRGMYZgwh"
   },
   "source": [
    "Same data loading as in Exercise 4.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAGOIQTiE6tr"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_subset = False # Set this to True for debugging purposes.\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "val_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "classes = train_dataset.classes\n",
    "\n",
    "if use_subset:\n",
    "    train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 100))\n",
    "    val_dataset = torch.utils.data.Subset(val_dataset, torch.arange(0, 100))\n",
    "\n",
    "print(f'classes: {classes}\\nnumber of instances:\\n\\ttrain: {len(train_dataset)}\\n\\tval: {len(val_dataset)}')\n",
    "\n",
    "def show_examples(n):\n",
    "    for i in range(n):\n",
    "        index = torch.randint(0, len(train_dataset), size=(1,)) # select a random example\n",
    "        image, target = train_dataset[index]\n",
    "        print(f'image of shape: {image.shape}')\n",
    "        print(f'label: {classes[target]}')\n",
    "        plt.imshow(image.permute(1,2,0).numpy())\n",
    "        plt.show()\n",
    "\n",
    "show_examples(4)\n",
    "\n",
    "batch_size = 32\n",
    "train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Optimizers\n",
    "\n",
    "Re-train the model for a maximum of $10$ epochs with\n",
    "* SGD without momentum\n",
    "* SGD with momentum (set to $0.9$)\n",
    "* AdaGrad\n",
    "* ADAM\n",
    "\n",
    "Use a learning rate of $0.001$. Make sure to fix the random seed before creating the model in each training run for a better comparison. To set the random seed, use `torch.manual_seed(0)` before intializing each model.\n",
    "\n",
    "For each epoch, print:\n",
    "* the average train loss\n",
    "* the average validation loss\n",
    "* the validation accuracy\n",
    "\n",
    "How do the optimizers compare in terms of convergence speed and resulting validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVCTY3gIE6ts"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Set up model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFi4DyArZgwi",
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def set_up_model():\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(3, 48, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(48, 96, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(96, 192, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(192*8*8 ,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "            )\n",
    "    return model\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train(epochs, model, loss_function, opt, train_loader, val_loader, evaluation):\n",
    "    res = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        cum_loss = 0\n",
    "        num_batches = 0\n",
    "        for img, label in tqdm(train_loader, desc='Train Iteration',ascii=True):\n",
    "            output = model(img)\n",
    "            loss = loss_function(output, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            cum_loss += loss.item()\n",
    "            num_batches +=1\n",
    "        acc, val_loss = evaluation(model, loss_function, val_loader)\n",
    "        cum_loss = cum_loss/len(train_loader)\n",
    "        res += [[cum_loss, val_loss, acc]]\n",
    "        print(f\"Epoch {epoch} \\t ----> \\t Loss {cum_loss:.5f} \\t ----- \\t Val Loss {val_loss:.5f} \\t ----- \\t Accuracy {acc:.5f}\")\n",
    "    return res\n",
    "\n",
    "\n",
    "def evaluation(model, loss_funct, val_loader):\n",
    "    model.eval()\n",
    "    total_matches = 0\n",
    "    val_entries = 0\n",
    "    cum_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(val_loader, desc='Val Iteration', ascii=True):\n",
    "            output = model(img)\n",
    "            loss = loss_funct(output, label)\n",
    "            cum_loss += loss.item()\n",
    "            prediction = torch.argmax(output, dim=1)\n",
    "            num_matches_batch = (prediction==label).sum()\n",
    "            total_matches += num_matches_batch\n",
    "            val_entries += len(img)\n",
    "    accuracy = total_matches/val_entries\n",
    "    loss = cum_loss/len(val_loader)\n",
    "    return accuracy.item(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run Training with the different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
    "sgd_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "sgd_mom_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "ada_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "adam_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "print(\"Train Loss\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[0]:4f} \\t {sgd_mom[0]:4f} \\t {ada[0]:4f} \\t {adam[0]:4f}\")\n",
    "    \n",
    "print(\"\\nVal Loss\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[1]:4f} \\t {sgd_mom[1]:4f} \\t {ada[1]:4f} \\t {adam[1]:4f}\")\n",
    "\n",
    "print(\"\\nAccuracy\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[2]:4f} \\t {sgd_mom[2]:4f} \\t {ada[2]:4f} \\t {adam[2]:4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Batch Normalization\n",
    "\n",
    "Repeat the experiment from 1., but add batch normalization after every convolutional layer (after the ReLU activation). What effect does adding batch normalization have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def set_up_model():\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(3, 48, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(48),\n",
    "            nn.Conv2d(48, 96, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Conv2d(96, 192, (3,3), padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.MaxPool2d((2,2)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(192*8*8 ,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10)\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "#ToDo: Run training with different optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.)\n",
    "sgd_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "sgd_mom_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "ada_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = set_up_model()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "adam_res = train(num_epochs, model, loss, optimizer, train_dl, val_dl, evaluation)\n",
    "\n",
    "print(\"Train Loss\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[0]:4f} \\t {sgd_mom[0]:4f} \\t {ada[0]:4f} \\t {adam[0]:4f}\")\n",
    "    \n",
    "print(\"\\nVal Loss\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[1]:4f} \\t {sgd_mom[1]:4f} \\t {ada[1]:4f} \\t {adam[1]:4f}\")\n",
    "\n",
    "print(\"\\nAccuracy\")\n",
    "print(\"Epoch \\t SGD    \\t SGD Momentum \\t AdaGrad \\t Adam\")\n",
    "for epoch, (sgd, sgd_mom, ada, adam) in enumerate(zip(sgd_res, sgd_mom_res, ada_res, adam_res)):\n",
    "    print(f\"{epoch} \\t {sgd[2]:4f} \\t {sgd_mom[2]:4f} \\t {ada[2]:4f} \\t {adam[2]:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "6_Optimization.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:dl_lai]",
   "language": "python",
   "name": "conda-env-dl_lai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
